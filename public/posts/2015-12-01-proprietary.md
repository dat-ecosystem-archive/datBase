# Where is the Iron Mountain for open data?
by [Karissa McKelvey](http://karissa.github.io)

**Proprietary services such as Google, GitHub, Dropbox, PLOS, Figshare, and AWS provide data hosting and sharing at a low cost. They've become the most popular platforms for data sharing in science. However, using them can be dangerous, because when the money runs out, so does your data.**

Imagine a scenario 30 years in the future, when most of the world's scientific data has been hosted on Dropbox...and the company closes its doors. They might give users a chance to download that data before it is deleted permanently, or they might not anticipate their demise to make that possible. Even if they do, and even if many people download their data, scientists do not have the same incentive to download and store their data for the simple purpose of archiving and reproducibility. Most scientists will not do this. The data will disappear.

This is already a serious problem _without_ a Dropbox closure, discovered when the question "Where are the original scripts and data you used?" is answered with "I don't know," or a referral to another person. Much of the content produced on these closed, proprietary systems will be deleted, with those valuable scientific artifacts lost forever.

Most of the companies around today won't exist in 50 years. The average lifespan of a company has decreased from 67 years in the 1920s to just [15 years today](http://www.bbc.com/news/business-16611040). If Galileo stored photos from his telescope on Dropbox in 1610, somebody would have had to re-host that data somewhere between 11 and 33 times for it to be available today. 
For decades, we've been copying paper records and storing them in literal [mountains of iron](http://www.ironmountain.com/) on microfilm, a hardy material that has [a life expectancy of a few hundred years](https://en.wikipedia.org/wiki/Microform). When information can be copied to other formats, it has higher redundancy, increasing the chance that it will be around forever. To this end, it should be made trivial to duplicate scientific data from a digital hosting service. When we want data to be around forever, it must be free to be retrieved and copied at any point in time.

## So how did we get here in the first place?

Unfortunately, many proprietary hosting services do not like it when their users can copy and re-host data. Many hosting companies will offer private installations on an organization's own hardware and databases. Even so, these clients may not copy, distribute, study, change, or improve the hosting software without permission from the company. This is how service hosting companies make money in the long run -- once the users buy into the software, it is difficult (or impossible) to move that data to another service. Proprietary services use code that is closed source for this purpose, so that users are incapable, legally, of running their own copy of the software and their data.

To make matters worse, even if the hosting service prides itself on its ease of data transfer, it can still take tremendous effort to move data off of a proprietary service. Amazon encourages users by making it free to _upload_ data to cloud storage, but it charges the organization every time the data needs to be _downloaded._ Bandwidth can become expensive for very large datasets. Who will pay for this data transfer cost? The individual researcher is unlikely to have a budget for that.

Although unfortunate, this market landscape is not necessarily the fault of companies, and a data service should not be considered harmful just because it is a business. I don't object to businesses running hosting services. But we need to start supporting and using services that are open source and free (as in speech, not beer) and allowing users to copy and re-host data. This should be considered not only a necessity, but an _inevitability_ in the use of the software. Even though this copying and re-hosting data may not good for all business models, it is a sure way to create redundancy and ensure that scientific materials will be around forever. The tension between profit and science must be resolved in favor of science.

## Start here: open data as common knowledge

As open data publication becomes normalized, we need to concern ourselves heavily with the method -- and that's where tools like Dat can come in. Dat is a distributed data sharing tool that is has been open source since day one. We have designed the tool explicitly for _editing, copying, re-hosting, and redistributing data._ The data hosting model will work like SETI@Home, where any user or organization can run a node that will offer bandwidth to the network to re-host and share data. This distribution model for data will enable libraries, non profits, scientific societies, universities, news organizations, and open data scientists to all participate in the open data network, drastically increasing redundancy and ensuring longevity compared to the current centralized model.

We're convinced that this distributed data model is the most durable for ensuring that public knowledge stays accessible. Brewster Kahle of the Internet Archive has called for [an internet that "locks" openness into how it works and operates](http://brewster.kahle.org/2015/08/11/locking-the-web-open-a-call-for-a-distributed-web-2/), by distributing data over a peer to peer network. We support this notion -- with a distributed, peer to peer data sharing tool, the power over the data is distributed. This method of distribution is harder to monetize by single corporate entity than a centralized, corporate model. The market simply does not have an incentive to build this kind of distributed database for public knowledge.

Libraries, on the other hand, are in a great position to be shepherds of public knowledge. In practice, copying scientific materials, such as books, results, drawings, sketches -- copying and making data retrievable -- is what libraries ought to be well-equipped to do. However, as scientific publishing has become more digital, many of us are challenged with the question of where to host open data. We are hoping that open data publishers choose open, redundant software. We can get far by simply helping librarians achieve their goals. Academic journals, universities, and software foundations should start listening to trained digital librarians who have advocated for these open systems by supporting their creation and using them instead. Let's stop using proprietary services for data publishing, and instead advocate for open, redundant systems.

Karissa McKelvey<br>
Multiple hat wearer and Developer, Dat Project<br>
[http://karissa.github.io](http://karissa.github.io)<br>[@captainkmac on Twitter](http://twitter.com/captainkmac)

- [1] *If you want to read more about this topic, I highly recommend reading the work of the late Eleanor Ostrom and Charlotte Hess on commons management, [Understanding the Knowledge Commons](https://mitpress.mit.edu/books/understanding-knowledge-commons).*
